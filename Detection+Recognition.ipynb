{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from matplotlib.patches import Rectangle\n",
    "from keras.models import load_model\n",
    "from matplotlib.patches import Circle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.spatial.distance import cosine\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model of MTCNN and FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = './model/facenet_keras.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aditya/anaconda3/lib/python3.7/site-packages/keras/engine/saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "detector=MTCNN()\n",
    "face_encoder = load_model(encoder_model)\n",
    "people_dir = './people'\n",
    "encoding_dict = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of face using FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode(face_encoder, face, size):\n",
    "    face = normalize(face)\n",
    "    face = cv2.resize(face, size)\n",
    "    encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]\n",
    "    return encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Face for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face(img, box):\n",
    "    x1, y1, width, height = box\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = img[y1:y2, x1:x2]\n",
    "    return face, (x1, y1), (x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    mean, std = img.mean(), img.std()\n",
    "    return (img - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_normalizer = Normalizer('l2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved Images whhich are encoded and stored in a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for person_name in os.listdir(people_dir):\n",
    "    person_dir = os.path.join(people_dir, person_name)\n",
    "    encodes = []\n",
    "    for img_name in os.listdir(person_dir):\n",
    "        img_path = os.path.join(person_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        results = detector.detect_faces(img)\n",
    "        if results:\n",
    "            res = max(results, key=lambda b: b['box'][2] * b['box'][3])\n",
    "            face, _, _ = get_face(img, res['box'])\n",
    "            \n",
    "            face = normalize(face)\n",
    "            face = cv2.resize(face,(160,160))\n",
    "            encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]\n",
    "            encodes.append(encode)\n",
    "    if encodes:\n",
    "        encode = np.sum(encodes, axis=0)\n",
    "        encode = l2_normalizer.transform(np.expand_dims(encode, axis=0))[0]\n",
    "        encoding_dict[person_name] = encode  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_t=0.6\n",
    "confidence_t=0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Face Detection and Recognition (Press q to end video stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "video =cv2.VideoCapture(0)\n",
    "present_candidates=[]\n",
    "\n",
    "while True:\n",
    "    check,frame=video.read()\n",
    "\n",
    "    faces=detector.detect_faces(frame)\n",
    "    \n",
    "    if faces !=[]:\n",
    "        for person in faces:\n",
    "            bounding_box=person[\"box\"]\n",
    "            keypoints=person[\"keypoints\"]\n",
    "#             if person['confidence'] < confidence_t:\n",
    "#                 continue\n",
    "            face, pt_1, pt_2 = get_face(frame, person['box'])\n",
    "            encode = get_encode(face_encoder, face,(160,160))\n",
    "            encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "            name = 'unknown'\n",
    "            \n",
    "            \n",
    "            distance = float(\"inf\")\n",
    "            \n",
    "            \n",
    "            for (db_name, db_enc) in encoding_dict.items():\n",
    "        \n",
    "                dist = cosine(db_enc, encode)\n",
    "          \n",
    "                if dist < recognition_t and dist < distance:\n",
    "                \n",
    "                    name = db_name\n",
    "                    distance = dist\n",
    "                    if name not in present_candidates:\n",
    "                        present_candidates.append(name)\n",
    "                    \n",
    "                    \n",
    "            if name == 'unknown':\n",
    "                cv2.rectangle(frame, pt_1, pt_2, (0, 0, 255), 2)\n",
    "                cv2.putText(frame,name, pt_1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "                \n",
    "            else:\n",
    "                cv2.rectangle(frame, pt_1, pt_2, (0, 255, 0), 2)\n",
    "                cv2.putText(frame,name + f'__{distance:.2f}', (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (0, 200, 200), 2)\n",
    "                \n",
    "\n",
    "       \n",
    "    cv2.imshow('frame',frame)  \n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "          break\n",
    "            \n",
    "            \n",
    "            \n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snap based Face Detection and recognition (Press Spacebar to take a snap and Escape to end the stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video1 =cv2.VideoCapture(0)\n",
    "present_candidates_1=[]\n",
    "img_counter=0\n",
    "while True:\n",
    "    \n",
    "    check,frame=video1.read()\n",
    "    \n",
    "    cv2.imshow(\"SS\",frame)\n",
    "    k=cv2.waitKey(1)\n",
    "    \n",
    "    if k%256 == 27:\n",
    "        print(\"Escape\")\n",
    "        break\n",
    "        \n",
    "    elif k%256==32:\n",
    "        faces=detector.detect_faces(frame)\n",
    "        if faces !=[]:\n",
    "            for person in faces:\n",
    "                bounding_box=person[\"box\"]\n",
    "                keypoints=person[\"keypoints\"]\n",
    "    #             if person['confidence'] < confidence_t:\n",
    "    #                 continue\n",
    "                face, pt_1, pt_2 = get_face(frame, person['box'])\n",
    "                encode = get_encode(face_encoder, face,(160,160))\n",
    "                encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "                name = 'unknown'\n",
    "\n",
    "\n",
    "                distance = float(\"inf\")\n",
    "\n",
    "\n",
    "                for (db_name, db_enc) in encoding_dict.items():\n",
    "\n",
    "                    dist = cosine(db_enc, encode)\n",
    "\n",
    "                    if dist < recognition_t and dist < distance:\n",
    "\n",
    "                        name = db_name\n",
    "                        distance = dist\n",
    "                        if name not in present_candidates_1:\n",
    "                            present_candidates_1.append(name)\n",
    "\n",
    "                if name == 'unknown':\n",
    "                    cv2.rectangle(frame, pt_1, pt_2, (0, 0, 255), 2)\n",
    "                    cv2.putText(frame,name, pt_1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "\n",
    "                else:\n",
    "                    cv2.rectangle(frame, pt_1, pt_2, (0, 255, 0), 2)\n",
    "                    cv2.putText(frame,name + f'__{distance:.2f}', (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                            (0, 200, 200), 2)\n",
    "                    \n",
    "        img_name=\"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name,frame)\n",
    "        print(\"Screenshot\")\n",
    "        img_counter+=1\n",
    "\n",
    "video1.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates present (Live video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aditya']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates present (Snap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aditya']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_candidates_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
