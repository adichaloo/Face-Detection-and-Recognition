{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "from matplotlib.patches import Rectangle\n",
    "from keras.models import load_model\n",
    "from matplotlib.patches import Circle\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.spatial.distance import cosine\n",
    "import os\n",
    "import datetime\n",
    "import xlwt\n",
    "import xlrd\n",
    "from xlwt import Workbook \n",
    "from xlutils.copy import copy\n",
    "import time\n",
    "# from facenet_pytorch import MTCNN\n",
    "# from cv2 import dnn_superres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model of MTCNN and FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = './model/facenet_keras.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "detector=MTCNN()\n",
    "face_encoder = load_model(encoder_model)\n",
    "people_dir = './people'\n",
    "encoding_dict = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'street sign', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'hat', 'backpack', 'umbrella', 'shoe', 'eye glasses', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'plate', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'mirror', 'dining table', 'window', 'desk', 'toilet', 'door', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'blender', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush', 'hair brush']\n"
     ]
    }
   ],
   "source": [
    "classNames = []\n",
    "with open('coco.names','r') as f:\n",
    "    classNames = f.read().splitlines()\n",
    "print(classNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dnn_Model 0x7f8dac3cf150>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightsPath = \"frozen_inference_graph.pb\"\n",
    "configPath = \"ssd_mobilenet_v3_large_coco_2020_01_14.pbtxt\"\n",
    "net = cv2.dnn_DetectionModel(weightsPath,configPath)\n",
    "net.setInputSize(320,320)\n",
    "net.setInputScale(1.0/ 127.5)\n",
    "net.setInputMean((127.5, 127.5, 127.5))\n",
    "net.setInputSwapRB(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sr = cv2.dnn_superres.DnnSuperResImpl_create()\n",
    "# path = \"LapSRN_x8.pb\"\n",
    "# sr.readModel(path)\n",
    "\n",
    "# # Set the desired model and scale to get correct pre- and post-processing\n",
    "# sr.setModel(\"lapsrn\", 8)\n",
    "\n",
    "# sr.setPreferableBackend(cv2.dnn.DNN_BACKEND_CUDA)\n",
    "# sr.setPreferableTarget(cv2.dnn.DNN_TARGET_CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding of face using FaceNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encode(face_encoder, face, size):\n",
    "    face = normalize(face)\n",
    "    face = cv2.resize(face, size)\n",
    "    encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]\n",
    "    return encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Face for encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_face(img, box):\n",
    "    x1, y1, width, height = box\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = img[y1:y2, x1:x2]\n",
    "    return face, (x1, y1), (x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(img):\n",
    "    mean, std = img.mean(), img.std()\n",
    "    return (img - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "l2_normalizer = Normalizer('l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export(a):\n",
    "    \n",
    "\n",
    "    ct = datetime.datetime.now()\n",
    "    \n",
    "    ws = xlrd.open_workbook('names_list.xls',formatting_info=True)\n",
    "    sheet = ws.sheet_by_index(0)\n",
    "    wb=copy(ws)\n",
    "    names_sheet=wb.get_sheet(0)\n",
    "    row_s=sheet.nrows\n",
    "    col_s=sheet.ncols\n",
    "    \n",
    "    names_sheet.write(0,col_s,str(ct.year)+\"_\"+str(ct.month)+\"_\"+str(ct.day)+\"_\"+str(ct.hour))\n",
    "    \n",
    "    for i in range(1,row_s):   \n",
    "        \n",
    "        if sheet.cell_value(i,0) in a:\n",
    "            names_sheet.write(i, col_s,\"P\")\n",
    "                \n",
    "        else:\n",
    "                \n",
    "            names_sheet.write(i, col_s,\"A\")\n",
    "    wb.save(\"names_list.xls\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_attendance(a):\n",
    "    \n",
    "\n",
    "    ct = datetime.datetime.now()\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    workbook = xlwt.Workbook()  \n",
    "\n",
    "    sheet = workbook.add_sheet(str(ct.year)+\"_\"+str(ct.month)+\"_\"+str(ct.day)) \n",
    "    sheet.write(0,0,\"Name\")\n",
    "\n",
    "\n",
    "    row = 1\n",
    "    col = 0\n",
    "    if len(a)>0:\n",
    "        for person_name in os.listdir(people_dir):\n",
    "            print(person_name)\n",
    "            print(a)\n",
    "\n",
    "            for x in range(0,len(a)):\n",
    "                if str(person_name) in a:\n",
    "                    sheet.write(row, col,     str(a[x]))\n",
    "                    sheet.write(row,col+1,\"P\")\n",
    "                    \n",
    "                else:\n",
    "                    sheet.write(row, col,     str(a[x]),)\n",
    "                    sheet.write(row,col+1,\"A\")\n",
    "                row+=1\n",
    "         \n",
    "            \n",
    "  \n",
    "        workbook.save(\"sample_class_1.xls\") \n",
    "    else:\n",
    "        sheet.write(1,0,\"No one is present\")\n",
    "        workbook.save(\"sample_class_1.xls\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saved Images whhich are encoded and stored in a Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ce026e9e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f8ce01f7560> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "for person_name in os.listdir(people_dir):\n",
    "    person_dir = os.path.join(people_dir, person_name)\n",
    "    encodes = []\n",
    "    for img_name in os.listdir(person_dir):\n",
    "        img_path = os.path.join(person_dir, img_name)\n",
    "        img = cv2.imread(img_path)\n",
    "\n",
    "        results = detector.detect_faces(img)\n",
    "#         results, probs= detector.detect(img)\n",
    "        \n",
    "        \n",
    "        \n",
    "        if results:\n",
    "            res = max(results, key=lambda b: b['box'][2] * b['box'][3])\n",
    "            face, _, _ = get_face(img, res['box'])\n",
    "            \n",
    "            face = normalize(face)\n",
    "            face = cv2.resize(face,(160,160))\n",
    "            encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]\n",
    "            encodes.append(encode)\n",
    "    if encodes:\n",
    "        encode = np.sum(encodes, axis=0)\n",
    "        encode = l2_normalizer.transform(np.expand_dims(encode, axis=0))[0]\n",
    "        encoding_dict[person_name] = encode  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "recognition_t=0.6\n",
    "confidence_t=0.99\n",
    "thres = 0.5 # Threshold to detect object\n",
    "nms_threshold = 0.2 #(0.1 to 1) 1 means no suppress , 0.1 means high suppress\n",
    "font = cv2.FONT_HERSHEY_PLAIN\n",
    "#font = cv2.FONT_HERSHEY_COMPLEX\n",
    "Colors = np.random.uniform(0, 255, size=(len(classNames), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Live Face Detection and Recognition (Press q to end video stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [279, 188, 185, 242], 'confidence': 0.9999814033508301, 'keypoints': {'left_eye': (337, 280), 'right_eye': (423, 284), 'nose': (379, 332), 'mouth_left': (338, 373), 'mouth_right': (413, 376)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [282, 189, 183, 235], 'confidence': 0.9999909400939941, 'keypoints': {'left_eye': (338, 280), 'right_eye': (424, 282), 'nose': (382, 331), 'mouth_left': (342, 372), 'mouth_right': (416, 375)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [284, 191, 181, 229], 'confidence': 0.9999995231628418, 'keypoints': {'left_eye': (339, 278), 'right_eye': (426, 281), 'nose': (383, 327), 'mouth_left': (342, 368), 'mouth_right': (415, 372)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [288, 193, 180, 222], 'confidence': 0.9999995231628418, 'keypoints': {'left_eye': (341, 278), 'right_eye': (425, 280), 'nose': (383, 324), 'mouth_left': (345, 367), 'mouth_right': (416, 370)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [285, 190, 178, 225], 'confidence': 0.9999887943267822, 'keypoints': {'left_eye': (342, 275), 'right_eye': (428, 279), 'nose': (385, 321), 'mouth_left': (342, 364), 'mouth_right': (415, 369)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [297, 191, 174, 217], 'confidence': 0.999996542930603, 'keypoints': {'left_eye': (354, 272), 'right_eye': (435, 277), 'nose': (395, 313), 'mouth_left': (354, 358), 'mouth_right': (424, 363)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [305, 192, 176, 221], 'confidence': 0.9999992847442627, 'keypoints': {'left_eye': (359, 271), 'right_eye': (441, 276), 'nose': (399, 313), 'mouth_left': (361, 356), 'mouth_right': (431, 361)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [300, 185, 170, 226], 'confidence': 0.99954754114151, 'keypoints': {'left_eye': (358, 272), 'right_eye': (440, 277), 'nose': (402, 319), 'mouth_left': (363, 363), 'mouth_right': (431, 366)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [279, 190, 182, 230], 'confidence': 0.9995028972625732, 'keypoints': {'left_eye': (338, 276), 'right_eye': (422, 281), 'nose': (382, 327), 'mouth_left': (342, 370), 'mouth_right': (411, 373)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [241, 195, 182, 226], 'confidence': 0.9998132586479187, 'keypoints': {'left_eye': (297, 283), 'right_eye': (379, 280), 'nose': (340, 333), 'mouth_left': (309, 376), 'mouth_right': (376, 373)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [240, 201, 179, 224], 'confidence': 0.9999985694885254, 'keypoints': {'left_eye': (293, 287), 'right_eye': (376, 284), 'nose': (336, 335), 'mouth_left': (303, 378), 'mouth_right': (372, 376)}}, {'box': [541, 256, 36, 43], 'confidence': 0.957345187664032, 'keypoints': {'left_eye': (552, 273), 'right_eye': (568, 270), 'nose': (562, 280), 'mouth_left': (557, 291), 'mouth_right': (570, 288)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [245, 198, 174, 224], 'confidence': 0.9999001026153564, 'keypoints': {'left_eye': (296, 285), 'right_eye': (378, 282), 'nose': (337, 331), 'mouth_left': (307, 375), 'mouth_right': (374, 372)}}, {'box': [536, 210, 36, 40], 'confidence': 0.9940872192382812, 'keypoints': {'left_eye': (547, 224), 'right_eye': (565, 224), 'nose': (556, 231), 'mouth_left': (547, 240), 'mouth_right': (564, 240)}}, {'box': [544, 242, 36, 42], 'confidence': 0.975214421749115, 'keypoints': {'left_eye': (555, 260), 'right_eye': (571, 256), 'nose': (564, 267), 'mouth_left': (561, 277), 'mouth_right': (573, 274)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [249, 201, 175, 219], 'confidence': 0.9999939203262329, 'keypoints': {'left_eye': (300, 290), 'right_eye': (381, 286), 'nose': (341, 339), 'mouth_left': (312, 379), 'mouth_right': (376, 378)}}, {'box': [542, 215, 36, 42], 'confidence': 0.9723561406135559, 'keypoints': {'left_eye': (553, 230), 'right_eye': (571, 231), 'nose': (562, 237), 'mouth_left': (553, 247), 'mouth_right': (570, 247)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [250, 197, 179, 230], 'confidence': 0.9997560381889343, 'keypoints': {'left_eye': (303, 285), 'right_eye': (385, 284), 'nose': (343, 336), 'mouth_left': (312, 376), 'mouth_right': (379, 377)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [252, 199, 177, 228], 'confidence': 0.9996289014816284, 'keypoints': {'left_eye': (307, 286), 'right_eye': (388, 285), 'nose': (348, 333), 'mouth_left': (316, 377), 'mouth_right': (380, 377)}}, {'box': [537, 186, 35, 40], 'confidence': 0.9930017590522766, 'keypoints': {'left_eye': (547, 200), 'right_eye': (564, 200), 'nose': (555, 208), 'mouth_left': (547, 216), 'mouth_right': (562, 216)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [257, 199, 176, 229], 'confidence': 0.9997761845588684, 'keypoints': {'left_eye': (311, 286), 'right_eye': (393, 286), 'nose': (353, 335), 'mouth_left': (319, 378), 'mouth_right': (385, 378)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [258, 202, 174, 223], 'confidence': 0.999893069267273, 'keypoints': {'left_eye': (313, 287), 'right_eye': (394, 287), 'nose': (355, 336), 'mouth_left': (319, 377), 'mouth_right': (387, 378)}}, {'box': [574, 231, 37, 43], 'confidence': 0.7917361259460449, 'keypoints': {'left_eye': (583, 248), 'right_eye': (600, 244), 'nose': (593, 255), 'mouth_left': (589, 265), 'mouth_right': (602, 263)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [260, 200, 178, 226], 'confidence': 0.9997819066047668, 'keypoints': {'left_eye': (315, 287), 'right_eye': (397, 287), 'nose': (357, 335), 'mouth_left': (322, 377), 'mouth_right': (389, 378)}}, {'box': [527, 234, 44, 54], 'confidence': 0.7597387433052063, 'keypoints': {'left_eye': (537, 258), 'right_eye': (555, 253), 'nose': (546, 266), 'mouth_left': (543, 278), 'mouth_right': (557, 274)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [261, 201, 177, 226], 'confidence': 0.9998421669006348, 'keypoints': {'left_eye': (316, 287), 'right_eye': (397, 288), 'nose': (358, 336), 'mouth_left': (323, 377), 'mouth_right': (390, 378)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [261, 198, 177, 228], 'confidence': 0.9997811913490295, 'keypoints': {'left_eye': (316, 285), 'right_eye': (399, 286), 'nose': (357, 335), 'mouth_left': (324, 377), 'mouth_right': (390, 378)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [266, 202, 178, 225], 'confidence': 0.9999651908874512, 'keypoints': {'left_eye': (322, 287), 'right_eye': (404, 287), 'nose': (365, 334), 'mouth_left': (328, 376), 'mouth_right': (396, 378)}}, {'box': [590, 243, 37, 43], 'confidence': 0.9825382232666016, 'keypoints': {'left_eye': (601, 260), 'right_eye': (617, 257), 'nose': (610, 268), 'mouth_left': (606, 278), 'mouth_right': (619, 275)}}, {'box': [583, 208, 37, 42], 'confidence': 0.9766808152198792, 'keypoints': {'left_eye': (594, 223), 'right_eye': (613, 224), 'nose': (603, 229), 'mouth_left': (594, 239), 'mouth_right': (611, 240)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [279, 210, 167, 206], 'confidence': 0.9999699592590332, 'keypoints': {'left_eye': (330, 291), 'right_eye': (407, 293), 'nose': (368, 339), 'mouth_left': (336, 375), 'mouth_right': (397, 377)}}, {'box': [592, 212, 39, 44], 'confidence': 0.9805144667625427, 'keypoints': {'left_eye': (604, 227), 'right_eye': (624, 227), 'nose': (615, 234), 'mouth_left': (605, 244), 'mouth_right': (622, 244)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [274, 200, 179, 230], 'confidence': 0.9998538494110107, 'keypoints': {'left_eye': (331, 286), 'right_eye': (416, 290), 'nose': (372, 336), 'mouth_left': (336, 377), 'mouth_right': (404, 382)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [263, 208, 174, 221], 'confidence': 0.9999887943267822, 'keypoints': {'left_eye': (322, 291), 'right_eye': (402, 293), 'nose': (364, 338), 'mouth_left': (327, 379), 'mouth_right': (394, 381)}}, {'box': [520, 229, 35, 41], 'confidence': 0.9912592768669128, 'keypoints': {'left_eye': (532, 244), 'right_eye': (549, 244), 'nose': (541, 250), 'mouth_left': (532, 260), 'mouth_right': (548, 260)}}, {'box': [527, 262, 38, 47], 'confidence': 0.8852233290672302, 'keypoints': {'left_eye': (537, 280), 'right_eye': (554, 276), 'nose': (547, 287), 'mouth_left': (543, 299), 'mouth_right': (555, 296)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [265, 212, 169, 216], 'confidence': 0.9999936819076538, 'keypoints': {'left_eye': (321, 293), 'right_eye': (401, 294), 'nose': (363, 338), 'mouth_left': (327, 380), 'mouth_right': (393, 382)}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [268, 209, 173, 218], 'confidence': 0.9999617338180542, 'keypoints': {'left_eye': (327, 293), 'right_eye': (407, 294), 'nose': (370, 337), 'mouth_left': (332, 378), 'mouth_right': (399, 380)}}, {'box': [548, 269, 37, 44], 'confidence': 0.9649778008460999, 'keypoints': {'left_eye': (558, 285), 'right_eye': (575, 283), 'nose': (567, 293), 'mouth_left': (563, 304), 'mouth_right': (576, 301)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [271, 203, 174, 224], 'confidence': 0.9998688697814941, 'keypoints': {'left_eye': (326, 289), 'right_eye': (409, 290), 'nose': (368, 337), 'mouth_left': (334, 378), 'mouth_right': (400, 380)}}, {'box': [547, 232, 38, 44], 'confidence': 0.9836770296096802, 'keypoints': {'left_eye': (558, 247), 'right_eye': (577, 248), 'nose': (567, 255), 'mouth_left': (558, 264), 'mouth_right': (574, 265)}}, {'box': [554, 269, 37, 43], 'confidence': 0.9448764324188232, 'keypoints': {'left_eye': (564, 285), 'right_eye': (580, 282), 'nose': (573, 293), 'mouth_left': (570, 303), 'mouth_right': (582, 301)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [277, 211, 172, 216], 'confidence': 0.9999769926071167, 'keypoints': {'left_eye': (334, 294), 'right_eye': (412, 298), 'nose': (376, 343), 'mouth_left': (339, 381), 'mouth_right': (404, 384)}}, {'box': [565, 271, 37, 44], 'confidence': 0.9581514000892639, 'keypoints': {'left_eye': (575, 289), 'right_eye': (592, 286), 'nose': (584, 298), 'mouth_left': (580, 307), 'mouth_right': (593, 305)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [276, 204, 176, 224], 'confidence': 0.9998173117637634, 'keypoints': {'left_eye': (334, 288), 'right_eye': (418, 291), 'nose': (377, 338), 'mouth_left': (339, 379), 'mouth_right': (406, 382)}}, {'box': [572, 275, 36, 43], 'confidence': 0.7870716452598572, 'keypoints': {'left_eye': (582, 291), 'right_eye': (598, 288), 'nose': (590, 298), 'mouth_left': (586, 308), 'mouth_right': (599, 306)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [72]]\n",
      "Faces===== [{'box': [285, 207, 170, 212], 'confidence': 0.9999347925186157, 'keypoints': {'left_eye': (341, 291), 'right_eye': (421, 297), 'nose': (382, 341), 'mouth_left': (342, 376), 'mouth_right': (407, 382)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [285, 203, 182, 231], 'confidence': 0.9999281167984009, 'keypoints': {'left_eye': (349, 289), 'right_eye': (432, 294), 'nose': (393, 342), 'mouth_left': (352, 382), 'mouth_right': (420, 387)}}]\n",
      "Classs Ids=== [[1]]\n",
      "Faces===== [{'box': [345, 212, 151, 193], 'confidence': 0.9974324107170105, 'keypoints': {'left_eye': (392, 288), 'right_eye': (467, 291), 'nose': (433, 331), 'mouth_left': (401, 368), 'mouth_right': (459, 373)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [307, 196, 185, 235], 'confidence': 0.9999942779541016, 'keypoints': {'left_eye': (368, 284), 'right_eye': (456, 288), 'nose': (414, 334), 'mouth_left': (371, 378), 'mouth_right': (443, 382)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [318, 189, 191, 247], 'confidence': 0.9999921321868896, 'keypoints': {'left_eye': (380, 283), 'right_eye': (471, 290), 'nose': (427, 340), 'mouth_left': (382, 381), 'mouth_right': (456, 387)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [308, 197, 188, 238], 'confidence': 0.9999967813491821, 'keypoints': {'left_eye': (371, 285), 'right_eye': (462, 290), 'nose': (416, 339), 'mouth_left': (372, 381), 'mouth_right': (446, 386)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [311, 194, 187, 242], 'confidence': 0.9999945163726807, 'keypoints': {'left_eye': (373, 285), 'right_eye': (462, 290), 'nose': (417, 339), 'mouth_left': (374, 381), 'mouth_right': (447, 386)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [310, 198, 187, 234], 'confidence': 0.9999978542327881, 'keypoints': {'left_eye': (375, 284), 'right_eye': (463, 290), 'nose': (422, 334), 'mouth_left': (375, 379), 'mouth_right': (448, 385)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [313, 198, 186, 236], 'confidence': 0.9999986886978149, 'keypoints': {'left_eye': (377, 286), 'right_eye': (465, 290), 'nose': (423, 339), 'mouth_left': (379, 382), 'mouth_right': (451, 386)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [313, 203, 181, 230], 'confidence': 0.9999971389770508, 'keypoints': {'left_eye': (375, 288), 'right_eye': (461, 290), 'nose': (421, 340), 'mouth_left': (378, 381), 'mouth_right': (449, 384)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [309, 204, 191, 242], 'confidence': 0.9999995231628418, 'keypoints': {'left_eye': (376, 292), 'right_eye': (463, 296), 'nose': (424, 347), 'mouth_left': (377, 387), 'mouth_right': (450, 391)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [309, 203, 186, 232], 'confidence': 0.9999992847442627, 'keypoints': {'left_eye': (372, 290), 'right_eye': (460, 293), 'nose': (420, 342), 'mouth_left': (376, 385), 'mouth_right': (447, 388)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [305, 177, 179, 226], 'confidence': 0.9999948740005493, 'keypoints': {'left_eye': (367, 261), 'right_eye': (452, 267), 'nose': (412, 305), 'mouth_left': (368, 350), 'mouth_right': (440, 356)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [43]]\n",
      "Faces===== [{'box': [311, 44, 177, 234], 'confidence': 0.9734491109848022, 'keypoints': {'left_eye': (374, 134), 'right_eye': (462, 138), 'nose': (419, 185), 'mouth_left': (380, 227), 'mouth_right': (451, 233)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [285, 107, 159, 200], 'confidence': 0.9999449253082275, 'keypoints': {'left_eye': (331, 184), 'right_eye': (405, 185), 'nose': (366, 236), 'mouth_left': (337, 268), 'mouth_right': (396, 269)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [285, 94, 174, 221], 'confidence': 0.9998816251754761, 'keypoints': {'left_eye': (339, 185), 'right_eye': (423, 187), 'nose': (380, 234), 'mouth_left': (341, 272), 'mouth_right': (408, 274)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [75]\n",
      " [77]]\n",
      "Faces===== [{'box': [297, 101, 167, 211], 'confidence': 0.9999936819076538, 'keypoints': {'left_eye': (351, 183), 'right_eye': (429, 186), 'nose': (393, 229), 'mouth_left': (355, 269), 'mouth_right': (417, 272)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [298, 98, 169, 218], 'confidence': 0.9998445510864258, 'keypoints': {'left_eye': (354, 181), 'right_eye': (436, 185), 'nose': (396, 224), 'mouth_left': (359, 268), 'mouth_right': (424, 271)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [299, 100, 168, 217], 'confidence': 0.999855637550354, 'keypoints': {'left_eye': (356, 183), 'right_eye': (437, 186), 'nose': (399, 228), 'mouth_left': (361, 271), 'mouth_right': (426, 274)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [297, 98, 171, 223], 'confidence': 0.9999068975448608, 'keypoints': {'left_eye': (355, 183), 'right_eye': (437, 188), 'nose': (398, 229), 'mouth_left': (358, 271), 'mouth_right': (426, 274)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [300, 101, 167, 219], 'confidence': 0.9997480511665344, 'keypoints': {'left_eye': (356, 183), 'right_eye': (437, 186), 'nose': (398, 228), 'mouth_left': (361, 272), 'mouth_right': (427, 274)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [295, 103, 174, 219], 'confidence': 0.9999626874923706, 'keypoints': {'left_eye': (356, 187), 'right_eye': (437, 190), 'nose': (400, 231), 'mouth_left': (358, 273), 'mouth_right': (426, 277)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [294, 110, 174, 219], 'confidence': 0.9999909400939941, 'keypoints': {'left_eye': (356, 194), 'right_eye': (437, 197), 'nose': (399, 238), 'mouth_left': (358, 280), 'mouth_right': (427, 283)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [299, 107, 170, 222], 'confidence': 0.9998764991760254, 'keypoints': {'left_eye': (357, 191), 'right_eye': (440, 193), 'nose': (402, 238), 'mouth_left': (363, 280), 'mouth_right': (429, 283)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [297, 105, 174, 224], 'confidence': 0.9998202919960022, 'keypoints': {'left_eye': (357, 190), 'right_eye': (442, 192), 'nose': (403, 239), 'mouth_left': (364, 281), 'mouth_right': (431, 283)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [299, 108, 169, 219], 'confidence': 0.9999780654907227, 'keypoints': {'left_eye': (359, 190), 'right_eye': (441, 194), 'nose': (403, 238), 'mouth_left': (363, 278), 'mouth_right': (428, 282)}}]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [298, 105, 173, 223], 'confidence': 0.9997124075889587, 'keypoints': {'left_eye': (358, 191), 'right_eye': (442, 193), 'nose': (404, 239), 'mouth_left': (364, 281), 'mouth_right': (431, 283)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [297, 103, 173, 220], 'confidence': 0.9999494552612305, 'keypoints': {'left_eye': (358, 187), 'right_eye': (439, 191), 'nose': (401, 231), 'mouth_left': (359, 273), 'mouth_right': (428, 277)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [75]\n",
      " [77]]\n",
      "Faces===== [{'box': [302, 136, 141, 175], 'confidence': 0.9999613761901855, 'keypoints': {'left_eye': (348, 204), 'right_eye': (414, 207), 'nose': (383, 235), 'mouth_left': (353, 273), 'mouth_right': (406, 276)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [75]\n",
      " [77]]\n",
      "Faces===== [{'box': [306, 141, 140, 184], 'confidence': 0.9996354579925537, 'keypoints': {'left_eye': (352, 210), 'right_eye': (419, 214), 'nose': (387, 248), 'mouth_left': (354, 283), 'mouth_right': (410, 286)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [75]\n",
      " [77]]\n",
      "Faces===== [{'box': [297, 159, 133, 176], 'confidence': 0.9999294281005859, 'keypoints': {'left_eye': (339, 223), 'right_eye': (403, 226), 'nose': (371, 261), 'mouth_left': (343, 295), 'mouth_right': (392, 297)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [75]\n",
      " [77]\n",
      " [78]]\n",
      "Faces===== [{'box': [272, 142, 153, 197], 'confidence': 0.9999359846115112, 'keypoints': {'left_eye': (331, 211), 'right_eye': (404, 223), 'nose': (368, 251), 'mouth_left': (326, 290), 'mouth_right': (385, 300)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]\n",
      " [78]]\n",
      "Faces===== [{'box': [211, 151, 170, 206], 'confidence': 0.9999945163726807, 'keypoints': {'left_eye': (270, 221), 'right_eye': (351, 220), 'nose': (316, 266), 'mouth_left': (275, 305), 'mouth_right': (341, 305)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [209, 146, 166, 210], 'confidence': 0.9999748468399048, 'keypoints': {'left_eye': (270, 226), 'right_eye': (348, 229), 'nose': (315, 271), 'mouth_left': (274, 310), 'mouth_right': (338, 312)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]\n",
      " [78]]\n",
      "Faces===== [{'box': [209, 149, 167, 207], 'confidence': 0.9999924898147583, 'keypoints': {'left_eye': (271, 227), 'right_eye': (349, 230), 'nose': (316, 272), 'mouth_left': (274, 309), 'mouth_right': (339, 312)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]\n",
      " [78]]\n",
      "Faces===== [{'box': [210, 149, 168, 213], 'confidence': 0.9999945163726807, 'keypoints': {'left_eye': (272, 230), 'right_eye': (351, 233), 'nose': (319, 276), 'mouth_left': (277, 316), 'mouth_right': (341, 319)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]\n",
      " [78]]\n",
      "Faces===== [{'box': [209, 149, 169, 213], 'confidence': 0.9999911785125732, 'keypoints': {'left_eye': (272, 230), 'right_eye': (350, 232), 'nose': (318, 275), 'mouth_left': (275, 315), 'mouth_right': (341, 318)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [207, 149, 170, 215], 'confidence': 0.9999926090240479, 'keypoints': {'left_eye': (271, 231), 'right_eye': (349, 233), 'nose': (317, 275), 'mouth_left': (276, 316), 'mouth_right': (340, 318)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [207, 146, 169, 217], 'confidence': 0.9999195337295532, 'keypoints': {'left_eye': (269, 229), 'right_eye': (348, 230), 'nose': (316, 275), 'mouth_left': (277, 316), 'mouth_right': (339, 317)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]]\n",
      "Faces===== [{'box': [207, 149, 167, 212], 'confidence': 0.9999610185623169, 'keypoints': {'left_eye': (269, 229), 'right_eye': (347, 231), 'nose': (316, 275), 'mouth_left': (276, 315), 'mouth_right': (338, 316)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]\n",
      " [78]]\n",
      "Faces===== [{'box': [207, 148, 167, 213], 'confidence': 0.9999524354934692, 'keypoints': {'left_eye': (268, 230), 'right_eye': (347, 231), 'nose': (315, 274), 'mouth_left': (276, 315), 'mouth_right': (338, 316)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]\n",
      " [78]]\n",
      "Faces===== [{'box': [205, 150, 168, 211], 'confidence': 0.9999786615371704, 'keypoints': {'left_eye': (268, 230), 'right_eye': (346, 231), 'nose': (314, 275), 'mouth_left': (274, 315), 'mouth_right': (338, 316)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]\n",
      " [78]]\n",
      "Faces===== [{'box': [205, 148, 167, 216], 'confidence': 0.9999133348464966, 'keypoints': {'left_eye': (267, 231), 'right_eye': (345, 232), 'nose': (314, 276), 'mouth_left': (274, 318), 'mouth_right': (337, 318)}}]\n",
      "Classs Ids=== [[ 1]\n",
      " [77]\n",
      " [78]]\n",
      "Faces===== [{'box': [203, 147, 170, 216], 'confidence': 0.9998918771743774, 'keypoints': {'left_eye': (265, 230), 'right_eye': (345, 231), 'nose': (311, 275), 'mouth_left': (272, 317), 'mouth_right': (337, 317)}}]\n"
     ]
    }
   ],
   "source": [
    "video =cv2.VideoCapture(0)\n",
    "present_candidates=[]\n",
    "\n",
    "fps_start_time = datetime.datetime.now()\n",
    "fps = 0\n",
    "total_frames = 0\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    check,frame=video.read()\n",
    "    total_frames = total_frames + 1\n",
    "#     frame=sr.upsample(frame)\n",
    "    \n",
    "    faces=detector.detect_faces(frame)\n",
    "    \n",
    "    classIds, confs, bbox = net.detect(frame,confThreshold=thres)\n",
    "    bbox = list(bbox)\n",
    "    confs = list(np.array(confs).reshape(1,-1)[0])\n",
    "    confs = list(map(float,confs))\n",
    "    \n",
    "    indices = cv2.dnn.NMSBoxes(bbox,confs,thres,nms_threshold)\n",
    "    \n",
    "    if len(classIds) != 0:\n",
    "        \n",
    "        for i in indices:\n",
    "            i = i[0]\n",
    "            box = bbox[i]\n",
    "            confidence = str(round(confs[i],2))\n",
    "            color = Colors[classIds[i][0]-1]\n",
    "            x,y,w,h = box[0],box[1],box[2],box[3]\n",
    "            cv2.rectangle(frame, (x,y), (x+w,y+h), color, thickness=2)\n",
    "            cv2.putText(frame, classNames[classIds[i][0]-1]+\" \"+confidence,(x+10,y+20),\n",
    "                        font,1,color,2)\n",
    "    \n",
    "    if faces !=[]:\n",
    "       \n",
    "        for person in faces:\n",
    "            bounding_box=person[\"box\"]\n",
    "            keypoints=person[\"keypoints\"]\n",
    "# #             if person['confidence'] < confidence_t:\n",
    "# #                 continue\n",
    "            face, pt_1, pt_2 = get_face(frame, person['box'])\n",
    "            cv2.rectangle(frame, pt_1 , pt_2, (0, 255, 0), 2)\n",
    "            encode = get_encode(face_encoder, face,(160,160))\n",
    "            encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "            name = 'unknown'\n",
    "            \n",
    "            \n",
    "            distance = float(\"inf\")\n",
    "            \n",
    "            \n",
    "            for (db_name, db_enc) in encoding_dict.items():\n",
    "        \n",
    "                dist = cosine(db_enc, encode)\n",
    "          \n",
    "                if dist < recognition_t and dist < distance:\n",
    "                \n",
    "                    name = db_name\n",
    "                    distance = dist\n",
    "                    if name not in present_candidates:\n",
    "                        present_candidates.append(name)\n",
    "                    \n",
    "                    \n",
    "            if name == 'unknown':\n",
    "                cv2.rectangle(frame, pt_1, pt_2, (0, 0, 255), 2)\n",
    "                cv2.putText(frame,name, pt_1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "                \n",
    "            else:\n",
    "    \n",
    "                cv2.rectangle(frame, pt_1 , pt_2, (0, 255, 0), 2)\n",
    "                cv2.putText(frame,name + f'__{distance:.2f}', (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                        (0, 200, 200), 2)\n",
    "\n",
    "                \n",
    "\n",
    "    fps_end_time = datetime.datetime.now()\n",
    "    time_diff = fps_end_time - fps_start_time\n",
    "    if time_diff.seconds == 0:\n",
    "        fps = 0.0\n",
    "    else:\n",
    "        fps = (total_frames / time_diff.seconds)\n",
    "\n",
    "    fps_text = \"FPS: {:.2f}\".format(fps)\n",
    "\n",
    "    cv2.putText(frame, fps_text, (5, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 1)\n",
    "    \n",
    "    cv2.imshow('frame',frame)  \n",
    "    if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "          break\n",
    "            \n",
    "            \n",
    "            \n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for no,obj in enumerate(classIds):\n",
    "    if obj==[1]:\n",
    "        count+=1\n",
    "        \n",
    "        \n",
    "print(count)#####Number of people detected "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Snap based Face Detection and recognition (Press Spacebar to take a snap and Escape to end the stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Escape\n"
     ]
    }
   ],
   "source": [
    "video1 =cv2.VideoCapture(0)\n",
    "present_candidates_1=[]\n",
    "img_counter=0\n",
    "fps_start_time = datetime.datetime.now()\n",
    "fps = 0\n",
    "total_frames = 0\n",
    "while True:\n",
    "    \n",
    "    check,frame=video1.read()\n",
    "    total_frames = total_frames + 1\n",
    "    \n",
    "    \n",
    "    fps_end_time = datetime.datetime.now()\n",
    "    time_diff = fps_end_time - fps_start_time\n",
    "    if time_diff.seconds == 0:\n",
    "        fps = 0.0\n",
    "    else:\n",
    "        fps = (total_frames / time_diff.seconds)\n",
    "\n",
    "    fps_text = \"FPS: {:.2f}\".format(fps)\n",
    "\n",
    "    cv2.putText(frame, fps_text, (5, 30), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (0, 0, 255), 1)\n",
    "    \n",
    "    cv2.imshow(\"SS\",frame)\n",
    "    k=cv2.waitKey(1)\n",
    "    \n",
    "    if k%256 == 27:\n",
    "        print(\"Escape\")\n",
    "        break\n",
    "        \n",
    "    elif k%256==32:\n",
    "        faces=detector.detect_faces(frame)\n",
    "        if faces !=[]:\n",
    "            for person in faces:\n",
    "                bounding_box=person[\"box\"]\n",
    "                keypoints=person[\"keypoints\"]\n",
    "    #             if person['confidence'] < confidence_t:\n",
    "    #                 continue\n",
    "                face, pt_1, pt_2 = get_face(frame, person['box'])\n",
    "                encode = get_encode(face_encoder, face,(160,160))\n",
    "                encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "                name = 'unknown'\n",
    "\n",
    "\n",
    "                distance = float(\"inf\")\n",
    "\n",
    "\n",
    "                for (db_name, db_enc) in encoding_dict.items():\n",
    "\n",
    "                    dist = cosine(db_enc, encode)\n",
    "\n",
    "                    if dist < recognition_t and dist < distance:\n",
    "\n",
    "                        name = db_name\n",
    "                        distance = dist\n",
    "                        if name not in present_candidates_1:\n",
    "                            present_candidates_1.append(name)\n",
    "\n",
    "                if name == 'unknown':\n",
    "                    cv2.rectangle(frame, pt_1, pt_2, (0, 0, 255), 2)\n",
    "                    cv2.putText(frame,name, pt_1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "\n",
    "                else:\n",
    "                    cv2.rectangle(frame, pt_1, pt_2, (0, 255, 0), 2)\n",
    "                    cv2.putText(frame,name + f'__{distance:.2f}', (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                            (0, 200, 200), 2)\n",
    "                    \n",
    "        \n",
    "                    \n",
    "        img_name=\"opencv_frame_{}.png\".format(img_counter)\n",
    "        cv2.imwrite(img_name,frame)\n",
    "        print(\"Screenshot\")\n",
    "        img_counter+=1\n",
    "\n",
    "video1.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates present (Live video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aditya']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates present (Snap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "present_candidates_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(present_candidates_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "export(present_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(faces)#Number of faces detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
